{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T10:53:59.256732Z","iopub.execute_input":"2022-02-08T10:53:59.257017Z","iopub.status.idle":"2022-02-08T10:53:59.275085Z","shell.execute_reply.started":"2022-02-08T10:53:59.256987Z","shell.execute_reply":"2022-02-08T10:53:59.273956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pycountry\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n# ML Libraries\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:53:59.277215Z","iopub.execute_input":"2022-02-08T10:53:59.278782Z","iopub.status.idle":"2022-02-08T10:54:02.799039Z","shell.execute_reply.started":"2022-02-08T10:53:59.27872Z","shell.execute_reply":"2022-02-08T10:54:02.798081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Dataset**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/omicron-rising/omicron.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:02.800188Z","iopub.execute_input":"2022-02-08T10:54:02.800404Z","iopub.status.idle":"2022-02-08T10:54:04.016502Z","shell.execute_reply.started":"2022-02-08T10:54:02.800378Z","shell.execute_reply":"2022-02-08T10:54:04.015586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Examining Dataset**","metadata":{}},{"cell_type":"code","source":"df.profile_report()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:04.018381Z","iopub.execute_input":"2022-02-08T10:54:04.018657Z","iopub.status.idle":"2022-02-08T10:54:48.022529Z","shell.execute_reply.started":"2022-02-08T10:54:04.018617Z","shell.execute_reply":"2022-02-08T10:54:48.018175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.024954Z","iopub.execute_input":"2022-02-08T10:54:48.025353Z","iopub.status.idle":"2022-02-08T10:54:48.047295Z","shell.execute_reply.started":"2022-02-08T10:54:48.025301Z","shell.execute_reply":"2022-02-08T10:54:48.04668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.048447Z","iopub.execute_input":"2022-02-08T10:54:48.048845Z","iopub.status.idle":"2022-02-08T10:54:48.136996Z","shell.execute_reply.started":"2022-02-08T10:54:48.048808Z","shell.execute_reply":"2022-02-08T10:54:48.13627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.138395Z","iopub.execute_input":"2022-02-08T10:54:48.138806Z","iopub.status.idle":"2022-02-08T10:54:48.187207Z","shell.execute_reply.started":"2022-02-08T10:54:48.138767Z","shell.execute_reply":"2022-02-08T10:54:48.186126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#total null values\ndf.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.188545Z","iopub.execute_input":"2022-02-08T10:54:48.189101Z","iopub.status.idle":"2022-02-08T10:54:48.27696Z","shell.execute_reply.started":"2022-02-08T10:54:48.189057Z","shell.execute_reply":"2022-02-08T10:54:48.275836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.isnull(df).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.280186Z","iopub.execute_input":"2022-02-08T10:54:48.280923Z","iopub.status.idle":"2022-02-08T10:54:48.363576Z","shell.execute_reply.started":"2022-02-08T10:54:48.280726Z","shell.execute_reply":"2022-02-08T10:54:48.36269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So from a preliminary analysis of our data we can see that the train set contains 78168 rows and 16 columns. Our data contains about 3.6% of missing values with them being in 'user_location', 'user_description' and 'hashtags' columns. There are 6 numerical, 8 categorical and 2 boolean columns.**","metadata":{}},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"# Missing Values","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,16))\nsns.displot(\n    data=df.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=3,\n    palette='BuGn'\n)\nplt.title('Bar plot showing Missing Values in training data', weight = 'bold', size = 20, color = 'black')\nplt.xlabel(\" \")\nplt.ylabel(\" \")\nplt.xticks(size = 12, weight = 'bold', color = 'black')\nplt.yticks(size = 12, weight = 'bold', color = 'black');\n\nplt.figure(figsize=(18,10))\nsns.heatmap(df.isna().transpose(),\n            cmap=\"copper\",\n            cbar_kws={'label': 'Missing Data'})\nplt.title('Heatmap showing Missing Values in training data', weight = 'bold', size = 20, color = 'brown')\nplt.xticks(size = 12, color = 'maroon')\nplt.yticks(size = 12, color = 'maroon')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:48.365043Z","iopub.execute_input":"2022-02-08T10:54:48.365968Z","iopub.status.idle":"2022-02-08T10:54:56.689434Z","shell.execute_reply.started":"2022-02-08T10:54:48.365917Z","shell.execute_reply":"2022-02-08T10:54:56.688326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation matrix","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(), square=True, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:56.690946Z","iopub.execute_input":"2022-02-08T10:54:56.691276Z","iopub.status.idle":"2022-02-08T10:54:56.994555Z","shell.execute_reply.started":"2022-02-08T10:54:56.691232Z","shell.execute_reply":"2022-02-08T10:54:56.993807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From this we can see that favourites and retweets are highly correlated which makes sense. Also user_verified and user_followers have a high correlation.**","metadata":{}},{"cell_type":"markdown","source":"# Some Other Plots","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:56.995897Z","iopub.execute_input":"2022-02-08T10:54:56.996998Z","iopub.status.idle":"2022-02-08T10:54:57.000658Z","shell.execute_reply.started":"2022-02-08T10:54:56.996957Z","shell.execute_reply":"2022-02-08T10:54:56.999827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change to date time format\ndf['date']=pd.to_datetime(df['date'])\n\n# tweets per hour\ntweets_per_hr = df['date'].dt.strftime('%H').value_counts().sort_index().to_frame(name='Count')\ntweets_per_hr['Hour']=tweets_per_hr.index\n\n# plot\nplt.figure(figsize=(12,7))\nax=sns.barplot(x='Hour', y='Count',data=tweets_per_hr, palette='inferno')\nax.bar_label(ax.containers[0])\nplt.title('Tweets per hour', size='xx-large')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:57.001888Z","iopub.execute_input":"2022-02-08T10:54:57.002106Z","iopub.status.idle":"2022-02-08T10:54:58.204377Z","shell.execute_reply.started":"2022-02-08T10:54:57.002079Z","shell.execute_reply":"2022-02-08T10:54:58.203535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# excluding null values (where location is not specified) in user_location \nlocation = [loc for loc in df['user_location'] if type(loc)==str]\n\n# extracting country names from given location\ncountry_name = [country.name for loc in location for country in pycountry.countries if country.name in loc]\ncountry_name[:5]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:54:58.206169Z","iopub.execute_input":"2022-02-08T10:54:58.20651Z","iopub.status.idle":"2022-02-08T10:55:11.084203Z","shell.execute_reply.started":"2022-02-08T10:54:58.206467Z","shell.execute_reply":"2022-02-08T10:55:11.08323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dictionary to count number of occurances of each country\ncount={}\nfor country in country_name:\n    count[country] = count.get(country, 0) + 1\n\n# Country vs tweets count\ncountry_df = pd.DataFrame({'Country': list(count.keys()),'Tweets Count': list(count.values())})\ncountry_df = country_df.sort_values(by = 'Tweets Count', ascending=False)\ncountry_df=country_df[:15] # top 15 countries\n\n# plot the data\nplt.figure(figsize=(20,8))\nplt.title('Country vs Tweets Count', size='xx-large')\nax = sns.barplot(x='Country', y='Tweets Count',data=country_df, palette='inferno', edgecolor='grey');\nax.bar_label(ax.containers[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:55:11.085637Z","iopub.execute_input":"2022-02-08T10:55:11.085893Z","iopub.status.idle":"2022-02-08T10:55:11.48309Z","shell.execute_reply.started":"2022-02-08T10:55:11.085863Z","shell.execute_reply":"2022-02-08T10:55:11.48166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def preprocess_tweet_text(tweet):\n    tweet.lower()\n    # Remove urls\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n    # Remove user @ references and '#' from tweet\n    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n    # Remove punctuations\n    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n    # lemmatization\n    tweet = [WordNetLemmatizer().lemmatize(word) for word in tweet.split(' ')]\n    tweet = \" \".join(tweet)\n    # stopword removal\n    tweet = [word for word in tweet.split(' ') if word not in set(stopwords.words('english'))]\n    tweet=\" \".join(tweet)\n    \n    return tweet","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:57:59.136167Z","iopub.execute_input":"2022-02-08T10:57:59.136462Z","iopub.status.idle":"2022-02-08T10:57:59.14441Z","shell.execute_reply.started":"2022-02-08T10:57:59.136432Z","shell.execute_reply":"2022-02-08T10:57:59.143193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(preprocess_tweet_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:58:03.524684Z","iopub.execute_input":"2022-02-08T10:58:03.524954Z","iopub.status.idle":"2022-02-08T11:01:29.853405Z","shell.execute_reply.started":"2022-02-08T10:58:03.524927Z","shell.execute_reply":"2022-02-08T11:01:29.852547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count = [len(text.split()) for text in df.text]\ndf['word_count'] = word_count\n\n# excluding text with less than 3 words\ndf=df[df['word_count']>2]\n\n# excluding tweets with more than 16 words\ndf=df[df['word_count']<17]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T11:02:46.0459Z","iopub.execute_input":"2022-02-08T11:02:46.04711Z","iopub.status.idle":"2022-02-08T11:02:46.234564Z","shell.execute_reply.started":"2022-02-08T11:02:46.047055Z","shell.execute_reply":"2022-02-08T11:02:46.233515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nSIA = SentimentIntensityAnalyzer()\n\ndf[\"Positive\"] = [SIA.polarity_scores(i)[\"pos\"] for i in df[\"text\"]]\ndf[\"Neutral\"] = [SIA.polarity_scores(j)[\"neu\"] for j in df[\"text\"]]\ndf[\"Negative\"] = [SIA.polarity_scores(k)[\"neg\"] for k in df[\"text\"]]\n\ndf1 = df[[\"text\", \"Positive\",\"Neutral\", \"Negative\"]]\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T11:03:40.792501Z","iopub.execute_input":"2022-02-08T11:03:40.792896Z","iopub.status.idle":"2022-02-08T11:04:23.508004Z","shell.execute_reply.started":"2022-02-08T11:03:40.792858Z","shell.execute_reply":"2022-02-08T11:04:23.507054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiments_nltk = []\n\nfor tweet in df.text:\n    sentiment_dict = SIA.polarity_scores(tweet)\n    sentiment_dict.pop('compound', None)\n    sentiments_nltk.append(max(sentiment_dict , key=sentiment_dict.get))\n    \ndf['sentiment_nltk'] = sentiments_nltk\ndf['sentiment_nltk'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T11:04:23.509656Z","iopub.execute_input":"2022-02-08T11:04:23.509909Z","iopub.status.idle":"2022-02-08T11:04:38.029044Z","shell.execute_reply.started":"2022-02-08T11:04:23.509879Z","shell.execute_reply":"2022-02-08T11:04:38.027986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T11:28:45.594193Z","iopub.execute_input":"2022-02-08T11:28:45.595982Z","iopub.status.idle":"2022-02-08T11:28:45.634306Z","shell.execute_reply.started":"2022-02-08T11:28:45.595883Z","shell.execute_reply":"2022-02-08T11:28:45.633341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiments_nltk","metadata":{"execution":{"iopub.status.busy":"2022-02-08T11:29:16.382484Z","iopub.execute_input":"2022-02-08T11:29:16.383798Z","iopub.status.idle":"2022-02-08T11:29:16.408335Z","shell.execute_reply.started":"2022-02-08T11:29:16.383747Z","shell.execute_reply":"2022-02-08T11:29:16.407628Z"},"trusted":true},"execution_count":null,"outputs":[]}]}